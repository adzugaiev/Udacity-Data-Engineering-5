## Udacity - Data Engineering - 5
# Data Pipelines with Airflow

## About / Synopsis

In this project demonstrates core concepts of Apache Airflow. I will create a DAG with custom operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.

## Table of Contents
* [Project Datasets](#project-datasets)
    - [Song Dataset](#song-dataset)
    - [Log Dataset](#log-dataset)
* [Airflow DAG](#airflow-dag)
    - [Stage Operator](#stage-operator)
    - [Fact and Dimension Operators](#fact-and-dimension-operators)
    - [Data Quality Operator](#data-quality-operator)
* [Target DWH Structure](#target-dwh-structure)
    - [Fact Table](#fact-table)
    - [Dimension Tables](#dimension-tables)
* [Files in the Project](#files-in-the-project)
* [Running the Project](#running-the-project)
* [What I Have Learned](#what-i-have-learned)
* [Author](#author)

## Project Datasets

I'll be working with two datasets that reside in S3. Here are the S3 links for each:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`
    - Log data json path: `s3://udacity-dend/log_json_path.json`

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, `song_data/A/B/C/TRABCEI128F424C983.json`

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, `log_data/2018/11/2018-11-12-events.json`.

## Airflow DAG

The DAG performs the following operations over the song and log datasets.

![DAG](../media/dag.png?raw=true)

### Stage Operator

The stage operator is expected to be able to load any JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. The operator's parameters should specify the S3 file key and the target table. The operator's template field allows it to load times tamped files from S3 based on the execution time and run backfills.

### Fact and Dimension Operators

Fact and dimension operators use SQL helper class to run data transformations. Most of the logic is within the SQL transformations and the operator is taking as input a SQL statement and target table that will contain the results of the transformation. Dimension operator also has a parameter that allows switching between insert modes when loading dimensions.

### Data Quality Operator

The data quality operator is running checks on the resulting data. The operator receives one or more SQL based test cases along with the expected results and executes them. For each test, the test result and expected result are checked and logged.

## Target DWH Structure

Using the song and log datasets, the DAG will create a star schema database warehouse.

### Fact Table

* **songplays** - records in log data associated with song plays i.e. records with page `NextSong`
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

### Dimension Tables

* **users** - users in the app
    - *user_id, first_name, last_name, gender, level*
* **songs** - songs in music database
    - *song_id, title, artist_id, year, duration*
* **artists** - artists in music database
    - *artist_id, name, location, latitude, longitude*
* **time** - timestamps of records in songplays broken down into specific units
    - *start_time, hour, day, week, month, year, weekday*

## Files in the Project

- `create_tables.sql` creates tables if they do not exist in `PostgresOperator`.
- `dags/sparkify_dag.py` defines Airflow DAG.
- `plugins/helpers/sql_queries.py` contains all SQL queries, and is imported into the DAG above.
- `plugins/operators/data_quality.py` custom operator `DataQualityOperator`.
- `plugins/operators/load_dimension.py` custom operator `LoadDimensionOperator`.
- `plugins/operators/load_fact.py` custom operator `LoadFactOperator`.
- `plugins/operators/stage_redshift.py` custom operator `StageToRedshiftOperator`.
- `README.md` provides the project description you are now reading.

## Running the Project

1) Copy project to your Airflow working folder `airflow`.
1) Create AWS Redshift cluster. A free trial cluster will suffice. Under security settings, enable the public access.
1) Open Airflow and create connection to AWS with name `aws_credentials` to access S3.
1) Create Airflow connection to Postgres with name `redshift`, use detail from your Redshift cluster.
1) In Airflow, locate DAG `sparkify` and enable it.

## What I Have Learned

Through the implementation of this project, while solving the project's core tasks, I've learned:

1) Setting up Airflow DAG.
1) Using Airflow operators, e.g. `PostgresOperator` running SQL from a file.
1) Implementing and using custom Airflow operators.
1) S3 to SQL ETL on Redshift with Airflow.

## Author

Andrii Dzugaiev, [in:dzugaev](https://www.linkedin.com/in/dzugaev/)